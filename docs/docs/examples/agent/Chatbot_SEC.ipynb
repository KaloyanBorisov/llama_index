{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c3c7da",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/Chatbot_SEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae56bcff",
   "metadata": {},
   "source": [
    "# ðŸ’¬ðŸ¤– How to Build a Chatbot\n",
    "\n",
    "LlamaIndex serves as a bridge between your data and Language Learning Models (LLMs), providing a toolkit that enables you to establish a query interface around your data for a variety of tasks, such as question-answering and summarization.\n",
    "\n",
    "In this tutorial, we'll walk you through building a context-augmented chatbot using a [Data Agent](https://gpt-index.readthedocs.io/en/stable/core_modules/agent_modules/agents/root.html). This agent, powered by LLMs, is capable of intelligently executing tasks over your data. The end result is a chatbot agent equipped with a robust set of data interface tools provided by LlamaIndex to answer queries about your data.\n",
    "\n",
    "**Note**: This tutorial builds upon initial work on creating a query interface over SEC 10-K filings - [check it out here](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d).\n",
    "\n",
    "### Context\n",
    "\n",
    "In this guide, weâ€™ll build a \"10-K Chatbot\" that uses raw UBER 10-K HTML filings from Dropbox. Users can interact with the chatbot to ask questions related to the 10-K filings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03f3e1de",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "35c20fbe",
   "metadata": {},
   "source": [
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-agent-openai\n",
    "%pip install llama-index-llms-openai"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1211059f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"....\"\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set text wrapping\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Add a default parameter\n",
    "def set_css(sender=None, *args, **kwargs):  \n",
    "    display(\n",
    "        HTML(\n",
    "            \"\"\"\n",
    "            <style>\n",
    "                pre {\n",
    "                    white-space: pre-wrap;\n",
    "                }\n",
    "            </style>\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "get_ipython().events.register(\"pre_run_cell\", set_css)"
   ],
   "id": "CuHeyb224pI2",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "218cc812",
   "metadata": {},
   "source": [
    "### Ingest Data\n",
    "\n",
    "Let's first download the raw 10-k files, from 2019-2022."
   ]
  },
  {
   "cell_type": "code",
   "id": "YC4R6nkCp91d",
   "metadata": {},
   "source": [
    "# NOTE: the code examples assume you're operating within a Jupyter notebook.\n",
    "# download files\n",
    "!mkdir data\n",
    "!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n",
    "!unzip data/UBER.zip -d data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2200f83",
   "metadata": {},
   "source": [
    "To parse the HTML files into formatted text, we use the [Unstructured](https://github.com/Unstructured-IO/unstructured) library. Thanks to [LlamaHub](https://llamahub.ai/), we can directly integrate with Unstructured, allowing conversion of any text into a Document format that LlamaIndex can ingest.\n",
    "\n",
    "First we install the necessary packages:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-magic-bin==0.4.14\n",
    "%pip install unstructured\n",
    "%pip install llama-index\n",
    "%pip install llama-index-readers-file"
   ],
   "id": "86fbd2db876a8c5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f55a00d7",
   "metadata": {},
   "source": [
    "Then we can use the `UnstructuredReader` to parse the HTML files into a list of `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "id": "5dcd0f94",
   "metadata": {},
   "source": [
    "from llama_index.readers.file import UnstructuredReader\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "years = [2022, 2021, 2020, 2019]\n",
    "loader = UnstructuredReader()\n",
    "doc_set = {}\n",
    "all_docs = []\n",
    "\n",
    "# Create base directory path\n",
    "base_path = Path(\"./data/UBER\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not base_path.exists():\n",
    "    print(f\"Directory {base_path} does not exist!\")\n",
    "    # Optionally create the directory\n",
    "    # base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for year in years:\n",
    "    file_path = base_path / f\"UBER_{year}.html\"\n",
    "\n",
    "    # Check if file exists before trying to load it\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            year_docs = loader.load_data(\n",
    "                file=file_path,\n",
    "                split_documents=False\n",
    "            )\n",
    "            # insert year metadata into each year\n",
    "            for d in year_docs:\n",
    "                d.metadata = {\"year\": year}\n",
    "            doc_set[year] = year_docs\n",
    "            all_docs.extend(year_docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file for year {year}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "312d0cfe",
   "metadata": {},
   "source": [
    "### Setting up Vector Indices for each year\n",
    "\n",
    "We first setup a vector index for each year. Each vector index allows us\n",
    "to ask questions about the 10-K filing of a given year.\n",
    "\n",
    "We build each index and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "id": "7c90fafc",
   "metadata": {},
   "source": [
    "# initialize simple vector indices\n",
    "# NOTE: don't run this cell if the indices are already loaded!\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 64\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "index_set = {}\n",
    "for year in years:\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    cur_index = VectorStoreIndex.from_documents(\n",
    "        doc_set[year],\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    index_set[year] = cur_index\n",
    "    storage_context.persist(persist_dir=f\"./storage/{year}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0704f6b",
   "metadata": {},
   "source": [
    "To load an index from disk, do the following"
   ]
  },
  {
   "cell_type": "code",
   "id": "7100e1b5",
   "metadata": {},
   "source": [
    "# Load indices from disk\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "index_set = {}\n",
    "for year in years:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=f\"./storage/{year}\"\n",
    "    )\n",
    "    cur_index = load_index_from_storage(\n",
    "        storage_context,\n",
    "    )\n",
    "    index_set[year] = cur_index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa3f903",
   "metadata": {},
   "source": [
    "### Setting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\n",
    "\n",
    "Since we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.\n",
    "\n",
    "To address this, we can use a [Sub Question Query Engine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/sub_question_query_engine.html). It decomposes a query into subqueries, each answered by an individual vector index, and synthesizes the results to answer the overall query.\n",
    "\n",
    "LlamaIndex provides some wrappers around indices (and query engines) so that they can be used by query engines and agents. First we define a `QueryEngineTool` for each vector index.\n",
    "Each tool has a name and a description; these are what the LLM agent sees to decide which tool to choose."
   ]
  },
  {
   "cell_type": "code",
   "id": "ce53419f",
   "metadata": {},
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "individual_query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=index_set[year].as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"vector_index_{year}\",\n",
    "            description=(\n",
    "                \"useful for when you want to answer queries about the\"\n",
    "                f\" {year} SEC 10-K for Uber\"\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    for year in years\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e8d2177",
   "metadata": {},
   "source": [
    "Now we can create the Sub Question Query Engine, which will allow us to synthesize answers across the 10-K filings. We pass in the `individual_query_engine_tools` we defined above."
   ]
  },
  {
   "cell_type": "code",
   "id": "9c6cee32",
   "metadata": {},
   "source": [
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=individual_query_engine_tools,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de5362b6",
   "metadata": {},
   "source": [
    "### Setting up the Chatbot Agent\n",
    "\n",
    "We use a LlamaIndex Data Agent to setup the outer chatbot agent, which has access to a set of Tools. Specifically, we will use an OpenAIAgent, that takes advantage of OpenAI API function calling. We want to use the separate Tools we defined previously for each index (corresponding to a given year), as well as a tool for the sub question query engine we defined above.\n",
    "\n",
    "First we define a `QueryEngineTool` for the sub question query engine:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f42e5a52",
   "metadata": {},
   "source": [
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"sub_question_query_engine\",\n",
    "        description=(\n",
    "            \"useful for when you want to answer queries that require analyzing\"\n",
    "            \" multiple SEC 10-K documents for Uber\"\n",
    "        ),\n",
    "    ),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdcc922d",
   "metadata": {},
   "source": [
    "Then, we combine the Tools we defined above into a single list of tools for the agent:"
   ]
  },
  {
   "cell_type": "code",
   "id": "fad25dca",
   "metadata": {},
   "source": [
    "tools = individual_query_engine_tools + [query_engine_tool]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14219225",
   "metadata": {},
   "source": [
    "Finally, we call `OpenAIAgent.from_tools` to create the agent, passing in the list of tools we defined above."
   ]
  },
  {
   "cell_type": "code",
   "id": "bb01833c",
   "metadata": {},
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "agent = OpenAIAgent.from_tools(tools, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e6112d4",
   "metadata": {},
   "source": [
    "### Testing the Agent\n",
    "\n",
    "We can now test the agent with various queries.\n",
    "\n",
    "If we test it with a simple \"hello\" query, the agent does not use any Tools."
   ]
  },
  {
   "cell_type": "code",
   "id": "269e6700",
   "metadata": {},
   "source": [
    "response = agent.chat(\"hi, i am bob\")\n",
    "print(str(response))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fe5fb92",
   "metadata": {},
   "source": [
    "If we test it with a query regarding the 10-k of a given year, the agent will use\n",
    "the relevant vector index Tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "bb8226e6",
   "metadata": {},
   "source": [
    "response = agent.chat(\n",
    "    \"What were some of the biggest risk factors in 2020 for Uber?\"\n",
    ")\n",
    "print(str(response))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78ac181f",
   "metadata": {},
   "source": [
    "Finally, if we test it with a query to compare/contrast risk factors across years, the agent will use the Sub Question Query Engine Tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "72e475bf",
   "metadata": {},
   "source": [
    "cross_query_str = (\n",
    "    \"Compare/contrast the risk factors described in the Uber 10-K across\"\n",
    "    \" years. Give answer in bullet points.\"\n",
    ")\n",
    "\n",
    "response = agent.chat(cross_query_str)\n",
    "print(str(response))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1153ee23",
   "metadata": {},
   "source": [
    "### Setting up the Chatbot Loop\n",
    "\n",
    "Now that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to chat with our SEC-augmented chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "id": "5fa14fa6",
   "metadata": {},
   "source": [
    "agent = OpenAIAgent.from_tools(tools)  # verbose=False by default\n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "    print(f\"Agent: {response}\")\n",
    "\n",
    "# User: What were some of the legal proceedings against Uber in 2022?"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
