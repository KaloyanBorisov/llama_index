{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8c3c7da",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/Chatbot_SEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae56bcff",
   "metadata": {},
   "source": [
    "# ðŸ’¬ðŸ¤– How to Build a Chatbot\n",
    "\n",
    "LlamaIndex serves as a bridge between your data and Language Learning Models (LLMs), providing a toolkit that enables you to establish a query interface around your data for a variety of tasks, such as question-answering and summarization.\n",
    "\n",
    "In this tutorial, we'll walk you through building a context-augmented chatbot using a [Data Agent](https://gpt-index.readthedocs.io/en/stable/core_modules/agent_modules/agents/root.html). This agent, powered by LLMs, is capable of intelligently executing tasks over your data. The end result is a chatbot agent equipped with a robust set of data interface tools provided by LlamaIndex to answer queries about your data.\n",
    "\n",
    "**Note**: This tutorial builds upon initial work on creating a query interface over SEC 10-K filings - [check it out here](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d).\n",
    "\n",
    "### Context\n",
    "\n",
    "In this guide, weâ€™ll build a \"10-K Chatbot\" that uses raw UBER 10-K HTML filings from Dropbox. Users can interact with the chatbot to ask questions related to the 10-K filings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03f3e1de",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "35c20fbe",
   "metadata": {},
   "source": [
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-agent-openai\n",
    "%pip install llama-index-llms-openai"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1211059f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"....\"\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set text wrapping\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Add a default parameter\n",
    "def set_css(sender=None, *args, **kwargs):  \n",
    "    display(\n",
    "        HTML(\n",
    "            \"\"\"\n",
    "            <style>\n",
    "                pre {\n",
    "                    white-space: pre-wrap;\n",
    "                }\n",
    "            </style>\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "get_ipython().events.register(\"pre_run_cell\", set_css)"
   ],
   "id": "CuHeyb224pI2",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "218cc812",
   "metadata": {},
   "source": [
    "### Ingest Data\n",
    "\n",
    "Let's first download the raw 10-k files, from 2019-2022."
   ]
  },
  {
   "cell_type": "code",
   "id": "YC4R6nkCp91d",
   "metadata": {},
   "source": [
    "# NOTE: the code examples assume you're operating within a Jupyter notebook.\n",
    "# download files\n",
    "!mkdir data\n",
    "!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n",
    "!unzip data/UBER.zip -d data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2200f83",
   "metadata": {},
   "source": [
    "To parse the HTML files into formatted text, we use the [Unstructured](https://github.com/Unstructured-IO/unstructured) library. Thanks to [LlamaHub](https://llamahub.ai/), we can directly integrate with Unstructured, allowing conversion of any text into a Document format that LlamaIndex can ingest.\n",
    "\n",
    "First we install the necessary packages:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install python-magic-bin==0.4.14\n",
    "%pip install unstructured\n",
    "%pip install llama-index\n",
    "%pip install llama-index-readers-file"
   ],
   "id": "86fbd2db876a8c5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f55a00d7",
   "metadata": {},
   "source": [
    "Then we can use the `UnstructuredReader` to parse the HTML files into a list of `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "id": "5dcd0f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:57:01.562621Z",
     "start_time": "2024-11-16T23:55:34.865153Z"
    }
   },
   "source": [
    "from llama_index.readers.file import UnstructuredReader\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "years = [2022, 2021, 2020, 2019]\n",
    "loader = UnstructuredReader()\n",
    "doc_set = {}\n",
    "all_docs = []\n",
    "\n",
    "# Create base directory path\n",
    "base_path = Path(\"./data/UBER\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not base_path.exists():\n",
    "    print(f\"Directory {base_path} does not exist!\")\n",
    "    # Optionally create the directory\n",
    "    # base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for year in years:\n",
    "    file_path = base_path / f\"UBER_{year}.html\"\n",
    "\n",
    "    # Check if file exists before trying to load it\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            year_docs = loader.load_data(\n",
    "                file=file_path,\n",
    "                split_documents=False\n",
    "            )\n",
    "            # insert year metadata into each year\n",
    "            for d in year_docs:\n",
    "                d.metadata = {\"year\": year}\n",
    "            doc_set[year] = year_docs\n",
    "            all_docs.extend(year_docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file for year {year}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\kaloq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\llama_index\\docs\\docs\\examples\\agent\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "312d0cfe",
   "metadata": {},
   "source": [
    "### Setting up Vector Indices for each year\n",
    "\n",
    "We first setup a vector index for each year. Each vector index allows us\n",
    "to ask questions about the 10-K filing of a given year.\n",
    "\n",
    "We build each index and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "id": "7c90fafc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:06:30.920193Z",
     "start_time": "2024-11-17T00:06:13.315258Z"
    }
   },
   "source": [
    "# initialize simple vector indices\n",
    "# NOTE: don't run this cell if the indices are already loaded!\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 64\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "index_set = {}\n",
    "for year in years:\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    cur_index = VectorStoreIndex.from_documents(\n",
    "        doc_set[year],\n",
    "        storage_context=storage_context,\n",
    "    )\n",
    "    index_set[year] = cur_index\n",
    "    storage_context.persist(persist_dir=f\"./storage/{year}\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0704f6b",
   "metadata": {},
   "source": [
    "To load an index from disk, do the following"
   ]
  },
  {
   "cell_type": "code",
   "id": "7100e1b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:07:21.308081Z",
     "start_time": "2024-11-17T00:07:16.235842Z"
    }
   },
   "source": [
    "# Load indices from disk\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "index_set = {}\n",
    "for year in years:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=f\"./storage/{year}\"\n",
    "    )\n",
    "    cur_index = load_index_from_storage(\n",
    "        storage_context,\n",
    "    )\n",
    "    index_set[year] = cur_index"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa3f903",
   "metadata": {},
   "source": [
    "### Setting up a Sub Question Query Engine to Synthesize Answers Across 10-K Filings\n",
    "\n",
    "Since we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.\n",
    "\n",
    "To address this, we can use a [Sub Question Query Engine](https://gpt-index.readthedocs.io/en/stable/examples/query_engine/sub_question_query_engine.html). It decomposes a query into subqueries, each answered by an individual vector index, and synthesizes the results to answer the overall query.\n",
    "\n",
    "LlamaIndex provides some wrappers around indices (and query engines) so that they can be used by query engines and agents. First we define a `QueryEngineTool` for each vector index.\n",
    "Each tool has a name and a description; these are what the LLM agent sees to decide which tool to choose."
   ]
  },
  {
   "cell_type": "code",
   "id": "ce53419f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:09:04.327360Z",
     "start_time": "2024-11-17T00:09:04.322544Z"
    }
   },
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "individual_query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=index_set[year].as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"vector_index_{year}\",\n",
    "            description=(\n",
    "                \"useful for when you want to answer queries about the\"\n",
    "                f\" {year} SEC 10-K for Uber\"\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    for year in years\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e8d2177",
   "metadata": {},
   "source": [
    "Now we can create the Sub Question Query Engine, which will allow us to synthesize answers across the 10-K filings. We pass in the `individual_query_engine_tools` we defined above."
   ]
  },
  {
   "cell_type": "code",
   "id": "9c6cee32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:09:27.017376Z",
     "start_time": "2024-11-17T00:09:26.672460Z"
    }
   },
   "source": [
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=individual_query_engine_tools,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de5362b6",
   "metadata": {},
   "source": [
    "### Setting up the Chatbot Agent\n",
    "\n",
    "We use a LlamaIndex Data Agent to setup the outer chatbot agent, which has access to a set of Tools. Specifically, we will use an OpenAIAgent, that takes advantage of OpenAI API function calling. We want to use the separate Tools we defined previously for each index (corresponding to a given year), as well as a tool for the sub question query engine we defined above.\n",
    "\n",
    "First we define a `QueryEngineTool` for the sub question query engine:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f42e5a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:10:00.117208Z",
     "start_time": "2024-11-17T00:10:00.111654Z"
    }
   },
   "source": [
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"sub_question_query_engine\",\n",
    "        description=(\n",
    "            \"useful for when you want to answer queries that require analyzing\"\n",
    "            \" multiple SEC 10-K documents for Uber\"\n",
    "        ),\n",
    "    ),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdcc922d",
   "metadata": {},
   "source": [
    "Then, we combine the Tools we defined above into a single list of tools for the agent:"
   ]
  },
  {
   "cell_type": "code",
   "id": "fad25dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:11:17.914894Z",
     "start_time": "2024-11-17T00:11:17.911173Z"
    }
   },
   "source": [
    "tools = individual_query_engine_tools + [query_engine_tool]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14219225",
   "metadata": {},
   "source": [
    "Finally, we call `OpenAIAgent.from_tools` to create the agent, passing in the list of tools we defined above."
   ]
  },
  {
   "cell_type": "code",
   "id": "bb01833c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:11:21.812344Z",
     "start_time": "2024-11-17T00:11:21.807503Z"
    }
   },
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "agent = OpenAIAgent.from_tools(tools, verbose=True)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e6112d4",
   "metadata": {},
   "source": [
    "### Testing the Agent\n",
    "\n",
    "We can now test the agent with various queries.\n",
    "\n",
    "If we test it with a simple \"hello\" query, the agent does not use any Tools."
   ]
  },
  {
   "cell_type": "code",
   "id": "269e6700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:11:29.043044Z",
     "start_time": "2024-11-17T00:11:27.807218Z"
    }
   },
   "source": [
    "response = agent.chat(\"hi, i am bob\")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: hi, i am bob\n",
      "Hello Bob! How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fe5fb92",
   "metadata": {},
   "source": [
    "If we test it with a query regarding the 10-k of a given year, the agent will use\n",
    "the relevant vector index Tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "bb8226e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:11:43.996843Z",
     "start_time": "2024-11-17T00:11:39.932343Z"
    }
   },
   "source": [
    "response = agent.chat(\n",
    "    \"What were some of the biggest risk factors in 2020 for Uber?\"\n",
    ")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What were some of the biggest risk factors in 2020 for Uber?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_index_2020 with args: {\"input\":\"biggest risk factors\"}\n",
      "Got output: The biggest risk factors include the adverse effects of the COVID-19 pandemic on the business, potential reclassification of Drivers, intense competition in the industries, significant losses and uncertain path to profitability, challenges in maintaining a critical mass of platform users, operational and cultural challenges, negative impact on brand reputation, difficulties in managing growth, safety incidents affecting platform users, financial impacts of the pandemic, and uncertainties related to the pandemic's future developments and its effects on business operations and financial results.\n",
      "========================\n",
      "\n",
      "In 2020, some of the biggest risk factors for Uber included the adverse effects of the COVID-19 pandemic on the business, potential reclassification of Drivers, intense competition in the industries, significant losses and uncertain path to profitability, challenges in maintaining a critical mass of platform users, operational and cultural challenges, negative impact on brand reputation, difficulties in managing growth, safety incidents affecting platform users, financial impacts of the pandemic, and uncertainties related to the pandemic's future developments and its effects on business operations and financial results.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78ac181f",
   "metadata": {},
   "source": [
    "Finally, if we test it with a query to compare/contrast risk factors across years, the agent will use the Sub Question Query Engine Tool."
   ]
  },
  {
   "cell_type": "code",
   "id": "72e475bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:12:49.084407Z",
     "start_time": "2024-11-17T00:12:38.796936Z"
    }
   },
   "source": [
    "cross_query_str = (\n",
    "    \"Compare/contrast the risk factors described in the Uber 10-K across\"\n",
    "    \" years. Give answer in bullet points.\"\n",
    ")\n",
    "\n",
    "response = agent.chat(cross_query_str)\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare/contrast the risk factors described in the Uber 10-K across years. Give answer in bullet points.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_index_2019 with args: {\"input\": \"risk factors\"}\n",
      "Got output: The personal mobility, meal delivery, and logistics industries are highly competitive, with well-established and low-cost alternatives that have been available for decades, low barriers to entry, low switching costs, and well-capitalized competitors in nearly every major geographic region. Conducting business internationally, particularly in countries with limited experience, exposes the company to risks such as operational and compliance challenges due to distance, language, and cultural differences, localization requirements, stricter laws and regulations compared to the United States, competition from local companies with better market understanding, varying levels of social acceptance and technological compatibility, legal uncertainties, currency exchange rate fluctuations, managing international operations, cash transaction preferences, tax implications, and financial reporting complexities.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: vector_index_2020 with args: {\"input\": \"risk factors\"}\n",
      "Got output: Some of the risk factors that could have an adverse effect on the business include the impact of the COVID-19 pandemic, potential reclassification of Drivers, intense competition in the industries, significant losses incurred since inception, challenges in attracting and maintaining platform users, operational and cultural challenges, negative media coverage affecting brand reputation, difficulties in managing growth, exposure to criminal or dangerous activities on the platform, legal uncertainties, managing international operations, fluctuations in currency exchange rates, regulatory challenges, tax consequences, financial reporting burdens, political and economic instability abroad, public health emergencies, and limited influence over minority-owned affiliates.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: vector_index_2021 with args: {\"input\": \"risk factors\"}\n",
      "Got output: The COVID-19 pandemic and the impact of actions to mitigate the pandemic, potential reclassification of Drivers, intense competition in the industries, significant losses incurred, challenges in maintaining brand reputation, workplace culture issues, safety incidents, and risks associated with substantial investments in new offerings and technologies are some of the key risk factors highlighted in the provided information.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: vector_index_2022 with args: {\"input\": \"risk factors\"}\n",
      "Got output: The business could be adversely affected if Drivers were classified as employees, workers, or quasi-employees instead of independent contractors.\n",
      "========================\n",
      "\n",
      "Here are the key risk factors described in the Uber 10-K documents across different years:\n",
      "\n",
      "2019:\n",
      "- Highly competitive industries with low barriers to entry and well-capitalized competitors\n",
      "- Risks associated with conducting business internationally, including operational and compliance challenges, legal uncertainties, and currency exchange rate fluctuations\n",
      "\n",
      "2020:\n",
      "- Adverse effects of the COVID-19 pandemic\n",
      "- Potential reclassification of Drivers\n",
      "- Intense competition in the industries\n",
      "- Significant losses incurred since inception\n",
      "- Challenges in attracting and maintaining platform users\n",
      "- Operational and cultural challenges\n",
      "- Negative media coverage affecting brand reputation\n",
      "- Difficulties in managing growth\n",
      "- Exposure to criminal or dangerous activities on the platform\n",
      "- Legal uncertainties and regulatory challenges\n",
      "\n",
      "2021:\n",
      "- Continued impact of the COVID-19 pandemic\n",
      "- Potential reclassification of Drivers\n",
      "- Intense competition in the industries\n",
      "- Significant losses incurred\n",
      "- Challenges in maintaining brand reputation\n",
      "- Workplace culture issues\n",
      "- Safety incidents\n",
      "- Risks associated with substantial investments in new offerings and technologies\n",
      "\n",
      "2022:\n",
      "- Adverse effects if Drivers were classified as employees, workers, or quasi-employees instead of independent contractors\n",
      "\n",
      "These risk factors highlight the evolving challenges and concerns faced by Uber over the years.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1153ee23",
   "metadata": {},
   "source": [
    "### Setting up the Chatbot Loop\n",
    "\n",
    "Now that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to chat with our SEC-augmented chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "id": "5fa14fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T00:14:39.795741Z",
     "start_time": "2024-11-17T00:14:04.260877Z"
    }
   },
   "source": [
    "agent = OpenAIAgent.from_tools(tools)  # verbose=False by default\n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "    print(f\"Agent: {response}\")\n",
    "\n",
    "# User: What were some of the legal proceedings against Uber in 2022?"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "Generated 2 sub questions.\n",
      "Generated 4 sub questions.\n",
      "Generated 4 sub questions.\n",
      "Agent: Here is a comparison of the risk factors described in the Uber 10-K across different years:\n",
      "\n",
      "- **2022**:\n",
      "  - The business could be adversely affected if Drivers were classified as employees, workers, or quasi-employees instead of independent contractors.\n",
      "\n",
      "- **2021**:\n",
      "  - Risk factors include the impact of the COVID-19 pandemic, potential reclassification of Drivers, intense competition in the industries, significant losses incurred, challenges in maintaining brand reputation, workplace culture issues, safety incidents, difficulties in managing growth, and risks associated with substantial investments in new technologies and offerings.\n",
      "\n",
      "- **2020**:\n",
      "  - Risk factors involve the impact of the COVID-19 pandemic, potential reclassification of Drivers, intense competition in the mobility and logistics industries, significant losses incurred since inception, challenges in attracting and maintaining platform users, operational and compliance challenges, negative media coverage affecting brand reputation, difficulties in managing growth, exposure to criminal or dangerous activities on the platform, legal uncertainties, difficulties in international operations, fluctuations in currency exchange rates, tax consequences, financial reporting burdens, political and economic instability, public health emergencies, and reduced protection for intellectual property rights in certain markets.\n",
      "\n",
      "- **2019**:\n",
      "  - The personal mobility, meal delivery, and logistics industries are highly competitive, with well-established and low-cost alternatives that have been available for decades, low barriers to entry, low switching costs, and well-capitalized competitors in nearly every major geographic region. Conducting business internationally exposes the company to risks such as operational and compliance challenges due to distance, language, and cultural differences, localization requirements, stricter laws and regulations compared to the United States, competition from local companies, varying levels of social acceptance and technological compatibility, legal uncertainties, currency exchange rate fluctuations, tax complexities, and financial reporting burdens.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m agent \u001B[38;5;241m=\u001B[39m OpenAIAgent\u001B[38;5;241m.\u001B[39mfrom_tools(tools)  \u001B[38;5;66;03m# verbose=False by default\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m----> 4\u001B[0m     text_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mUser: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m text_input \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
